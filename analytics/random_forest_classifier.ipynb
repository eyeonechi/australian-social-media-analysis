{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['SPARK_HOME'] = \"spark\"\n",
    "sys.path.append(\"spark/python\")\n",
    "sys.path.append(\"spark/python/lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coordinates of a given city\n",
    "import httplib2\n",
    "import json\n",
    "\n",
    "\n",
    "def cityPos(name):\n",
    "    url = \"https://maps.googleapis.com/maps/api/geocode/json?\" + \\\n",
    "          \"key=AIzaSyBsZErhxaT1oVgMrT-xGLcAN5nK3UHeGBU&address=\" + name\n",
    "    req = httplib2.Http(\".cache\")\n",
    "    resp, content = req.request(url, \"GET\")\n",
    "    res = json.loads(content)\n",
    "    return res[\"results\"][0][\"geometry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reform the data preparing for fitting the model\n",
    "import csv\n",
    "import json\n",
    "import codecs\n",
    "import time\n",
    "from couch import Couch\n",
    "\n",
    "COUCHDB_NAME = \"classified2\"\n",
    "REFORMED_FILE = \"data/output0.csv\"\n",
    "\n",
    "food_dict = {}\n",
    "rev_dict = {}\n",
    "\n",
    "def trans(path):\n",
    "    con = Couch(COUCHDB_NAME)\n",
    "    jsonData = con.query_all()\n",
    "\n",
    "    csvfile = open(REFORMED_FILE, 'w', newline='')\n",
    "    writer = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_ALL)\n",
    "    keys=['id', 'time', 'lat', 'lng', 'polarity', 'followers', 'following', 'food']\n",
    "    writer.writerow(keys)\n",
    "    \n",
    "    i = 0\n",
    "    for dic in jsonData:\n",
    "        if dic['location']['coordinates'] is None:\n",
    "            city = dic['location']['place_name']\n",
    "            city = city.replace(\" \",\"%20\")\n",
    "            coor = cityPos(city)\n",
    "            lng = coor['location']['lng']\n",
    "            lat = coor['location']['lat']\n",
    "        else:\n",
    "            lng = dic['location']['coordinates'][0]\n",
    "            lat = dic['location']['coordinates'][1]\n",
    "            \n",
    "        dt = dic['created_at']['year']+'-'+trans_month(dic['created_at']['month'])+'-'+dic['created_at']['day']+\\\n",
    "                ' '+dic['created_at']['time']\n",
    "        timeArray = time.strptime(dt, \"%Y-%m-%d %H:%M:%S\")\n",
    "        timestamp = time.mktime(timeArray)\n",
    "        \n",
    "        foods = dic['food_list']\n",
    "        if foods is None or len(foods) == 0:\n",
    "            writer.writerow([i, timestamp, lat, lng, dic['polarity'], dic['user']['followers'], dic['user']\\\n",
    "                                 ['following'], \"-1\"])\n",
    "        else:\n",
    "            for food in foods:\n",
    "                food_class = get_food_class(food)\n",
    "                writer.writerow([i, timestamp, lat, lng, dic['polarity'], dic['user']['followers'], dic['user']\\\n",
    "                                 ['following'], food_class])\n",
    "        i += 1\n",
    "    csvfile.close()\n",
    "    \n",
    "def trans_month(month):\n",
    "    month_dic = {'Jan': '1', 'Feb': '2', 'Mar': '3', 'Apr': '4', 'May': '5', 'Jun': '6', \\\n",
    "                 'Jul': '7', 'Aug': '8', 'Sep': '9', 'Oct': '10', 'Nov': '11', 'Dec': '12'}\n",
    "    return month_dic[month]\n",
    "\n",
    "def get_food_class(food):\n",
    "    if not food in food_dict.keys():\n",
    "        food_dict[food] = str(len(food_dict))\n",
    "    return food_dict[food]\n",
    "\n",
    "def get_rev_dict():\n",
    "    for key,value in food_dict.items():\n",
    "        rev_dict[value] = key\n",
    "\n",
    "def get_food_type(food_class):\n",
    "    the_class = str(int(food_class))\n",
    "    if the_class in rev_dict.keys():\n",
    "        return rev_dict[the_class]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"random forest test\"\n",
    "SPARK_URL = \"local[*]\"\n",
    "RANDOM_SEED = 12345\n",
    "TRAINING_DATA_RATIO = 0.7\n",
    "RF_NUM_TREES = 3\n",
    "RF_MAX_DEPTH = 4\n",
    "RF_NUM_BINS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(APP_NAME) \\\n",
    "    .master(SPARK_URL) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/duer/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: DeprecationWarning: generator 'CouchDatabase.__iter__' raised StopIteration\n"
     ]
    }
   ],
   "source": [
    "# read data from couchdb and reform them into a dataframe\n",
    "trans(REFORMED_FILE)\n",
    "\n",
    "df = spark.read \\\n",
    "    .options(header = \"true\", inferschema = \"true\") \\\n",
    "    .csv(REFORMED_FILE)\n",
    "\n",
    "print(\"Total number of rows: %d\" % df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-----------+--------+---------+---------+----+\n",
      "| id|         time|         lat|        lng|polarity|followers|following|food|\n",
      "+---+-------------+------------+-----------+--------+---------+---------+----+\n",
      "|  0|1.524673187E9| -37.7760072|144.9708071|  0.8883|      380|      564|   0|\n",
      "|  1|1.524710653E9| -33.8688197|151.2092955|  0.4019|      766|      552|   0|\n",
      "|  2|1.524712536E9| -37.8136276|144.9630576|     0.0|      853|      558|   0|\n",
      "|  3|1.524713009E9|-33.86642251|151.2012542|     0.0|       37|      169|   0|\n",
      "|  4|1.524699928E9|    -28.0183|   153.3921|   0.926|       84|      143|   0|\n",
      "|  5|1.524705058E9| -33.8688197|151.2092955| -0.2484|      154|      219|   0|\n",
      "|  5|1.524705058E9| -33.8688197|151.2092955| -0.2484|      154|      219|   1|\n",
      "|  6|1.524675517E9|   -33.88888|  151.27759|     0.0|     4111|     4550|   0|\n",
      "|  7| 1.52489073E9| -37.8136276|144.9630576| -0.3078|       93|      477|   0|\n",
      "|  8|1.524941428E9| -33.8688197|151.2092955|  0.7543|      168|      206|   0|\n",
      "|  9|1.524942922E9|    -37.8823|      144.7|   0.296|      363|      472|   0|\n",
      "|  9|1.524942922E9|    -37.8823|      144.7|   0.296|      363|      472|   2|\n",
      "| 10|1.524920039E9| -37.4713077|144.7851531|  0.8674|      805|      608|   0|\n",
      "| 11|1.524920248E9|  -28.016667|      153.4|  0.1999|     1831|     2342|   0|\n",
      "| 12|1.524952166E9|  -27.999481|  153.42798|  0.6369|     2617|     1081|   0|\n",
      "| 13|1.524934916E9| -37.8136276|144.9630576|     0.0|     3965|     4321|  -1|\n",
      "| 14|1.524935043E9| -37.8136276|144.9630576|   0.658|      849|     3987|  -1|\n",
      "| 15|1.524935842E9| -37.8136276|144.9630576|  0.8807|      804|      805|   0|\n",
      "| 16|1.524875467E9| -33.8688197|151.2092955|  0.4005|      154|      384|   0|\n",
      "| 17|1.524875673E9| -38.1353835|145.8487441|  0.3169|      105|      326|   0|\n",
      "+---+-------------+------------+-----------+--------+---------+---------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @@@@@@@@@@@@@@@@@@@ test @@@@@@@@@@@@@@@@@@@ -- begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+\n",
      "|emp_id|    name|age|\n",
      "+------+--------+---+\n",
      "|     1|    John| 25|\n",
      "|     2|     Ray| 35|\n",
      "|     3|    Mike| 24|\n",
      "|     4|    Jane| 28|\n",
      "|     5|   Kevin| 26|\n",
      "|     6| Vincent| 35|\n",
      "|     7|   James| 38|\n",
      "|     8|   Shane| 32|\n",
      "|     9|   Larry| 29|\n",
      "|    10|Kimberly| 29|\n",
      "|    11|    Alex| 28|\n",
      "|    12|   Garry| 25|\n",
      "|    13|     Max| 31|\n",
      "+------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees = [(1, \"John\", 25), (2, \"Ray\", 35), (3,\"Mike\", 24), (4, \"Jane\", 28), \n",
    "             (5, \"Kevin\", 26), \n",
    "             (6, \"Vincent\", 35), (7,\"James\", 38), (8, \"Shane\", 32), \n",
    "             (9, \"Larry\", 29), (10, \"Kimberly\", 29),\n",
    "             (11, \"Alex\", 28), (12, \"Garry\", 25), (13, \"Max\",31)]\n",
    "employees=spark.createDataFrame(employees, schema=[\"emp_id\",\"name\",\"age\"])\n",
    "employees.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|emp_id|salary|\n",
      "+------+------+\n",
      "|     1|  1000|\n",
      "|     2|  2000|\n",
      "|     3|  3000|\n",
      "|     4|  4000|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salary=[(1,1000),(2,2000),(3,3000),(4,4000)]\n",
    "salary=spark.createDataFrame(salary, schema=[\"emp_id\",\"salary\"])\n",
    "salary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|emp_id|departement|\n",
      "+------+-----------+\n",
      "|     1|       1000|\n",
      "|     2|       2000|\n",
      "|     3|       3000|\n",
      "|     4|       4000|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department=[(1,1000),(2,2000),(3,3000),(4,4000)]\n",
    "department=spark.createDataFrame(department, schema=[\"emp_id\",\"departement\"])\n",
    "department.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+------+-----------+\n",
      "|emp_id|    name|age|salary|departement|\n",
      "+------+--------+---+------+-----------+\n",
      "|     7|   James| 38|  null|       null|\n",
      "|     6| Vincent| 35|  null|       null|\n",
      "|     9|   Larry| 29|  null|       null|\n",
      "|     5|   Kevin| 26|  null|       null|\n",
      "|     1|    John| 25|  1000|       1000|\n",
      "|    10|Kimberly| 29|  null|       null|\n",
      "|     3|    Mike| 24|  3000|       3000|\n",
      "|    12|   Garry| 25|  null|       null|\n",
      "|     8|   Shane| 32|  null|       null|\n",
      "|    11|    Alex| 28|  null|       null|\n",
      "|     2|     Ray| 35|  2000|       2000|\n",
      "|     4|    Jane| 28|  4000|       4000|\n",
      "|    13|     Max| 31|  null|       null|\n",
      "+------+--------+---+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data = employees.join(salary, on='emp_id', how='left')\\\n",
    "    .join(department, on='emp_id', how='left')\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @@@@@@@@@@@@@@@@@@@ test @@@@@@@@@@@@@@@@@@@ -- end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows without food: 3\n",
      "number of rows with food: 46\n"
     ]
    }
   ],
   "source": [
    "# filter dataframe based on whether the data in them have food or not\n",
    "df_no_food = df.filter(df['food'] == -1)\n",
    "df_with_food = df.filter(df['food'] >= 0)\n",
    "\n",
    "print(\"Number of rows without food: %d\" % df_no_food.count())\n",
    "print(\"number of rows with food: %d\" % df_with_food.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training set rows: 35\n",
      "Number of test set rows: 11\n"
     ]
    }
   ],
   "source": [
    "# transform dataframe into RDD and split reformed data into tranning data and test data\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "transformed_df = df_with_food.rdd.map(lambda row: LabeledPoint(row[-1], Vectors.dense(row[1:-1])))\n",
    "\n",
    "splits = [TRAINING_DATA_RATIO, 1.0 - TRAINING_DATA_RATIO]\n",
    "training_data, test_data = transformed_df.randomSplit(splits, RANDOM_SEED)\n",
    "\n",
    "print(\"Number of training set rows: %d\" % training_data.count())\n",
    "print(\"Number of test set rows: %d\" % test_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train model: 0.296 seconds\n"
     ]
    }
   ],
   "source": [
    "# train the model using training data\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from time import *\n",
    "\n",
    "start_time = time()\n",
    "num_classes = len(food_dict)\n",
    "\n",
    "model = RandomForest.trainClassifier(training_data, numClasses=num_classes, categoricalFeaturesInfo={}, \\\n",
    "    numTrees=RF_NUM_TREES, featureSubsetStrategy=\"auto\", impurity=\"gini\", \\\n",
    "    maxDepth=RF_MAX_DEPTH, maxBins=32, seed=RANDOM_SEED)\n",
    "\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time to train model: %.3f seconds\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 81.818%\n"
     ]
    }
   ],
   "source": [
    "# make predictions using test data and calculate the accuracy\n",
    "predictions = model.predict(test_data.map(lambda x: x.features))\n",
    "labels_and_predictions = test_data.map(lambda x: x.label).zip(predictions)\n",
    "acc = labels_and_predictions.filter(lambda x: x[0] == x[1]).count() / float(test_data.count())\n",
    "print(\"Model accuracy: %.3f%%\" % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with data without food\n",
    "transformed_df_no_food = df_no_food.rdd.map(lambda row: LabeledPoint(row[-1], Vectors.dense(row[1:-1])))\n",
    "predict_foods = model.predict(transformed_df_no_food.map(lambda x: x.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine id with predicted food class\n",
    "rdd_predict_foods = df_no_food.rdd.map(lambda row: row[0]).zip(predict_foods.map(int))\n",
    "list_predict_food = rdd_predict_foods.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----------+-----------+--------+---------+---------+----+\n",
      "| id|         time|        lat|        lng|polarity|followers|following|food|\n",
      "+---+-------------+-----------+-----------+--------+---------+---------+----+\n",
      "| 13|1.524934916E9|-37.8136276|144.9630576|     0.0|     3965|     4321|   0|\n",
      "| 14|1.524935043E9|-37.8136276|144.9630576|   0.658|      849|     3987|   0|\n",
      "| 30|1.524928576E9|-37.8136276|144.9630576|  0.9366|     1444|      664|   0|\n",
      "+---+-------------+-----------+-----------+--------+---------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transform predicted rdd to dataframe and join it to original data that without food\n",
    "df_predict_foods = spark.createDataFrame(list_predict_food, schema=[\"id\",\"food\"])\n",
    "df_no_food = df_no_food.drop('food')\n",
    "\n",
    "concat_df = df_no_food.join(df_predict_foods, on='id')\n",
    "concat_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-----------+--------+---------+---------+----+\n",
      "| id|         time|         lat|        lng|polarity|followers|following|food|\n",
      "+---+-------------+------------+-----------+--------+---------+---------+----+\n",
      "|  0|1.524673187E9| -37.7760072|144.9708071|  0.8883|      380|      564|   0|\n",
      "|  1|1.524710653E9| -33.8688197|151.2092955|  0.4019|      766|      552|   0|\n",
      "|  2|1.524712536E9| -37.8136276|144.9630576|     0.0|      853|      558|   0|\n",
      "|  3|1.524713009E9|-33.86642251|151.2012542|     0.0|       37|      169|   0|\n",
      "|  4|1.524699928E9|    -28.0183|   153.3921|   0.926|       84|      143|   0|\n",
      "|  5|1.524705058E9| -33.8688197|151.2092955| -0.2484|      154|      219|   0|\n",
      "|  5|1.524705058E9| -33.8688197|151.2092955| -0.2484|      154|      219|   1|\n",
      "|  6|1.524675517E9|   -33.88888|  151.27759|     0.0|     4111|     4550|   0|\n",
      "|  7| 1.52489073E9| -37.8136276|144.9630576| -0.3078|       93|      477|   0|\n",
      "|  8|1.524941428E9| -33.8688197|151.2092955|  0.7543|      168|      206|   0|\n",
      "|  9|1.524942922E9|    -37.8823|      144.7|   0.296|      363|      472|   0|\n",
      "|  9|1.524942922E9|    -37.8823|      144.7|   0.296|      363|      472|   2|\n",
      "| 10|1.524920039E9| -37.4713077|144.7851531|  0.8674|      805|      608|   0|\n",
      "| 11|1.524920248E9|  -28.016667|      153.4|  0.1999|     1831|     2342|   0|\n",
      "| 12|1.524952166E9|  -27.999481|  153.42798|  0.6369|     2617|     1081|   0|\n",
      "| 15|1.524935842E9| -37.8136276|144.9630576|  0.8807|      804|      805|   0|\n",
      "| 16|1.524875467E9| -33.8688197|151.2092955|  0.4005|      154|      384|   0|\n",
      "| 17|1.524875673E9| -38.1353835|145.8487441|  0.3169|      105|      326|   0|\n",
      "| 18|1.524877085E9| -36.8875485|149.9058748|  0.7845|      110|      142|   0|\n",
      "| 19|1.524877478E9| -31.9505269|115.8604572|  0.4215|     1919|      288|   0|\n",
      "+---+-------------+------------+-----------+--------+---------+---------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# two dataframes: df_with_food and concat_df\n",
    "df_with_food.show()\n",
    "#concat_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----------+--------+---------+---------+----+\n",
      "|         time|        lat|        lng|polarity|followers|following|food|\n",
      "+-------------+-----------+-----------+--------+---------+---------+----+\n",
      "|1.524934916E9|-37.8136276|144.9630576|     0.0|     3965|     4321|   0|\n",
      "|1.524935043E9|-37.8136276|144.9630576|   0.658|      849|     3987|   0|\n",
      "|1.524928576E9|-37.8136276|144.9630576|  0.9366|     1444|      664|   0|\n",
      "+-------------+-----------+-----------+--------+---------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reform the dataframe to prepare for tranforming to json\n",
    "concat_df = concat_df.drop('id')\n",
    "concat_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"id\":13,\"time\":1.524934916E9,\"lat\":-37.8136276,\"lng\":144.9630576,\"polarity\":0.0,\"followers\":3965,\"following\":4321,\"food\":0}'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_df.toJSON().first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under Precision/Recall (PR) curve: 5\n",
      "Area under Receiver Operating Characteristic (ROC) curve: 45.000\n",
      "Time to evaluate model: 0.183 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "metrics = BinaryClassificationMetrics(labels_and_predictions)\n",
    "print(\"Area under Precision/Recall (PR) curve: %.f\" % (metrics.areaUnderPR * 100))\n",
    "print(\"Area under Receiver Operating Characteristic (ROC) curve: %.3f\" % (metrics.areaUnderROC * 100))\n",
    "\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time to evaluate model: %.3f seconds\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
