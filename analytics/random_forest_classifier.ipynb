{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['SPARK_HOME'] = \"spark\"\n",
    "sys.path.append(\"spark/python\")\n",
    "sys.path.append(\"spark/python/lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coordinates of a given city\n",
    "import httplib2\n",
    "import json\n",
    "\n",
    "\n",
    "def cityPos(name):\n",
    "    url = \"https://maps.googleapis.com/maps/api/geocode/json?\" + \\\n",
    "          \"key=AIzaSyBsZErhxaT1oVgMrT-xGLcAN5nK3UHeGBU&address=\" + name\n",
    "    req = httplib2.Http(\".cache\")\n",
    "    resp, content = req.request(url, \"GET\")\n",
    "    res = json.loads(content)\n",
    "    return res[\"results\"][0][\"geometry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reform the data preparing for fitting the model\n",
    "import csv\n",
    "import json\n",
    "import codecs\n",
    "import time as t\n",
    "from couch import Couch\n",
    "\n",
    "COUCHDB_NAME = \"classified2\"\n",
    "REFORMED_FILE = \"data/output0.csv\"\n",
    "\n",
    "food_dict = {}\n",
    "rev_dict = {}\n",
    "\n",
    "def trans(path):\n",
    "    con = Couch(COUCHDB_NAME)\n",
    "    jsonData = con.query_all()\n",
    "\n",
    "    csvfile = open(REFORMED_FILE, 'w', newline='')\n",
    "    writer = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_ALL)\n",
    "    keys=['id', 'time', 'timestamp', 'lat', 'lng', 'polarity', 'followers', 'following', 'food_class']\n",
    "    writer.writerow(keys)\n",
    "    \n",
    "    i = 0\n",
    "    for dic in jsonData:\n",
    "        if dic['location']['coordinates'] is None:\n",
    "            city = dic['location']['place_name']\n",
    "            city = city.replace(\" \",\"%20\")\n",
    "            coor = cityPos(city)\n",
    "            lng = coor['location']['lng']\n",
    "            lat = coor['location']['lat']\n",
    "        else:\n",
    "            lng = dic['location']['coordinates'][0]\n",
    "            lat = dic['location']['coordinates'][1]\n",
    "            \n",
    "        time = dic['created_at']['day']+'-'+trans_month(dic['created_at']['month'])+'-'+dic['created_at']['year']+\\\n",
    "                ' '+dic['created_at']['time']\n",
    "        timeArray = t.strptime(time, \"%d-%m-%Y %H:%M:%S\")\n",
    "        timestamp = t.mktime(timeArray)\n",
    "        \n",
    "        foods = dic['food_list']\n",
    "        if foods is None or len(foods) == 0:\n",
    "            writer.writerow([i, time, timestamp, lat, lng, dic['polarity'], dic['user']['followers'], dic['user']\\\n",
    "                                 ['following'], \"-1\"])\n",
    "        else:\n",
    "            for food in foods:\n",
    "                food_class = get_food_class(food)\n",
    "                writer.writerow([i, time, timestamp, lat, lng, dic['polarity'], dic['user']['followers'], dic['user']\\\n",
    "                                 ['following'], food_class])\n",
    "        i += 1\n",
    "    csvfile.close()\n",
    "    \n",
    "def trans_month(month):\n",
    "    month_dic = {'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06', \\\n",
    "                 'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'}\n",
    "    return month_dic[month]\n",
    "\n",
    "def get_food_class(food):\n",
    "    if not food in food_dict.keys():\n",
    "        food_dict[food] = str(len(food_dict))\n",
    "    return food_dict[food]\n",
    "\n",
    "def generate_rev_dict():\n",
    "    for key,value in food_dict.items():\n",
    "        rev_dict[value] = key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"random forest test\"\n",
    "SPARK_URL = \"local[*]\"\n",
    "RANDOM_SEED = 12345\n",
    "TRAINING_DATA_RATIO = 0.7\n",
    "RF_NUM_TREES = 3\n",
    "RF_MAX_DEPTH = 4\n",
    "RF_NUM_BINS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(APP_NAME) \\\n",
    "    .master(SPARK_URL) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/duer/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: DeprecationWarning: generator 'CouchDatabase.__iter__' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 49\n"
     ]
    }
   ],
   "source": [
    "# read data from couchdb and reform them into a dataframe\n",
    "trans(REFORMED_FILE)\n",
    "\n",
    "df = spark.read \\\n",
    "    .options(header = \"true\", inferschema = \"true\") \\\n",
    "    .csv(REFORMED_FILE)\n",
    "\n",
    "print(\"Total number of rows: %d\" % df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------+------------+-----------+--------+---------+---------+----------+\n",
      "| id|               time|    timestamp|         lat|        lng|polarity|followers|following|food_class|\n",
      "+---+-------------------+-------------+------------+-----------+--------+---------+---------+----------+\n",
      "|  0|26-04-2018 02:19:47|1.524673187E9| -37.7760072|144.9708071|  0.8883|      380|      564|         0|\n",
      "|  1|26-04-2018 12:44:13|1.524710653E9| -33.8688197|151.2092955|  0.4019|      766|      552|         0|\n",
      "|  2|26-04-2018 13:15:36|1.524712536E9| -37.8136276|144.9630576|     0.0|      853|      558|         0|\n",
      "|  3|26-04-2018 13:23:29|1.524713009E9|-33.86642251|151.2012542|     0.0|       37|      169|         0|\n",
      "|  4|26-04-2018 09:45:28|1.524699928E9|    -28.0183|   153.3921|   0.926|       84|      143|         0|\n",
      "|  5|26-04-2018 11:10:58|1.524705058E9| -33.8688197|151.2092955| -0.2484|      154|      219|         0|\n",
      "|  5|26-04-2018 11:10:58|1.524705058E9| -33.8688197|151.2092955| -0.2484|      154|      219|         1|\n",
      "|  6|26-04-2018 02:58:37|1.524675517E9|   -33.88888|  151.27759|     0.0|     4111|     4550|         0|\n",
      "|  7|28-04-2018 14:45:30| 1.52489073E9| -37.8136276|144.9630576| -0.3078|       93|      477|         0|\n",
      "|  8|29-04-2018 04:50:28|1.524941428E9| -33.8688197|151.2092955|  0.7543|      168|      206|         0|\n",
      "|  9|29-04-2018 05:15:22|1.524942922E9|    -37.8823|      144.7|   0.296|      363|      472|         0|\n",
      "|  9|29-04-2018 05:15:22|1.524942922E9|    -37.8823|      144.7|   0.296|      363|      472|         2|\n",
      "| 10|28-04-2018 22:53:59|1.524920039E9| -37.4713077|144.7851531|  0.8674|      805|      608|         0|\n",
      "| 11|28-04-2018 22:57:28|1.524920248E9|  -28.016667|      153.4|  0.1999|     1831|     2342|         0|\n",
      "| 12|29-04-2018 07:49:26|1.524952166E9|  -27.999481|  153.42798|  0.6369|     2617|     1081|         0|\n",
      "| 13|29-04-2018 03:01:56|1.524934916E9| -37.8136276|144.9630576|     0.0|     3965|     4321|        -1|\n",
      "| 14|29-04-2018 03:04:03|1.524935043E9| -37.8136276|144.9630576|   0.658|      849|     3987|        -1|\n",
      "| 15|29-04-2018 03:17:22|1.524935842E9| -37.8136276|144.9630576|  0.8807|      804|      805|         0|\n",
      "| 16|28-04-2018 10:31:07|1.524875467E9| -33.8688197|151.2092955|  0.4005|      154|      384|         0|\n",
      "| 17|28-04-2018 10:34:33|1.524875673E9| -38.1353835|145.8487441|  0.3169|      105|      326|         0|\n",
      "+---+-------------------+-------------+------------+-----------+--------+---------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows without food: 3\n",
      "number of rows with food: 46\n"
     ]
    }
   ],
   "source": [
    "# filter dataframe based on whether the data in them have food or not\n",
    "df_no_food = df.filter(df['food_class'] == -1)\n",
    "df_with_food = df.filter(df['food_class'] >= 0)\n",
    "\n",
    "print(\"Number of rows without food: %d\" % df_no_food.count())\n",
    "print(\"number of rows with food: %d\" % df_with_food.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training set rows: 35\n",
      "Number of test set rows: 11\n"
     ]
    }
   ],
   "source": [
    "# transform dataframe into RDD and split reformed data into tranning data and test data\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "transformed_df = df_with_food.rdd.map(lambda row: LabeledPoint(row[-1], Vectors.dense(row[2:-1])))\n",
    "\n",
    "splits = [TRAINING_DATA_RATIO, 1.0 - TRAINING_DATA_RATIO]\n",
    "training_data, test_data = transformed_df.randomSplit(splits, RANDOM_SEED)\n",
    "\n",
    "print(\"Number of training set rows: %d\" % training_data.count())\n",
    "print(\"Number of test set rows: %d\" % test_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train model: 0.811 seconds\n"
     ]
    }
   ],
   "source": [
    "# train the model using training data\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from time import *\n",
    "\n",
    "start_time = time()\n",
    "num_classes = len(food_dict)\n",
    "\n",
    "model = RandomForest.trainClassifier(training_data, numClasses=num_classes, categoricalFeaturesInfo={}, \\\n",
    "    numTrees=RF_NUM_TREES, featureSubsetStrategy=\"auto\", impurity=\"gini\", \\\n",
    "    maxDepth=RF_MAX_DEPTH, maxBins=32, seed=RANDOM_SEED)\n",
    "\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time to train model: %.3f seconds\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 81.818%\n"
     ]
    }
   ],
   "source": [
    "# make predictions using test data and calculate the accuracy\n",
    "predictions = model.predict(test_data.map(lambda x: x.features))\n",
    "labels_and_predictions = test_data.map(lambda x: x.label).zip(predictions)\n",
    "acc = labels_and_predictions.filter(lambda x: x[0] == x[1]).count() / float(test_data.count())\n",
    "print(\"Model accuracy: %.3f%%\" % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with data without food\n",
    "transformed_df_no_food = df_no_food.rdd.map(lambda row: LabeledPoint(row[-1], Vectors.dense(row[2:-1])))\n",
    "predict_foods = model.predict(transformed_df_no_food.map(lambda x: x.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine id with predicted food class\n",
    "rdd_predict_foods = df_no_food.rdd.map(lambda row: row[0]).zip(predict_foods.map(int))\n",
    "list_predict_food = rdd_predict_foods.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------+-----------+-----------+--------+---------+---------+----------+\n",
      "| id|               time|    timestamp|        lat|        lng|polarity|followers|following|food_class|\n",
      "+---+-------------------+-------------+-----------+-----------+--------+---------+---------+----------+\n",
      "| 13|29-04-2018 03:01:56|1.524934916E9|-37.8136276|144.9630576|     0.0|     3965|     4321|         0|\n",
      "| 14|29-04-2018 03:04:03|1.524935043E9|-37.8136276|144.9630576|   0.658|      849|     3987|         0|\n",
      "| 30|29-04-2018 01:16:16|1.524928576E9|-37.8136276|144.9630576|  0.9366|     1444|      664|         0|\n",
      "+---+-------------------+-------------+-----------+-----------+--------+---------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transform predicted rdd to dataframe and join it to original data that without food\n",
    "df_predict_foods = spark.createDataFrame(list_predict_food, schema=[\"id\",\"food_class\"])\n",
    "df_no_food = df_no_food.drop('food_class')\n",
    "\n",
    "concat_df = df_no_food.join(df_predict_foods, on='id')\n",
    "concat_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two dataframes: df_with_food and concat_df\n",
    "#df_with_food.show()\n",
    "#concat_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "+-------------------+------------+-----------+--------+---------+---------+---------+\n",
      "|               time|         lat|        lng|polarity|followers|following|     food|\n",
      "+-------------------+------------+-----------+--------+---------+---------+---------+\n",
      "|29-04-2018 03:01:56| -37.8136276|144.9630576|     0.0|     3965|     4321|    pizza|\n",
      "|29-04-2018 03:04:03| -37.8136276|144.9630576|   0.658|      849|     3987|    pizza|\n",
      "|29-04-2018 01:16:16| -37.8136276|144.9630576|  0.9366|     1444|      664|    pizza|\n",
      "|26-04-2018 02:19:47| -37.7760072|144.9708071|  0.8883|      380|      564|    pizza|\n",
      "|26-04-2018 12:44:13| -33.8688197|151.2092955|  0.4019|      766|      552|    pizza|\n",
      "|26-04-2018 13:15:36| -37.8136276|144.9630576|     0.0|      853|      558|    pizza|\n",
      "|26-04-2018 13:23:29|-33.86642251|151.2012542|     0.0|       37|      169|    pizza|\n",
      "|26-04-2018 09:45:28|    -28.0183|   153.3921|   0.926|       84|      143|    pizza|\n",
      "|26-04-2018 11:10:58| -33.8688197|151.2092955| -0.2484|      154|      219|    pizza|\n",
      "|26-04-2018 11:10:58| -33.8688197|151.2092955| -0.2484|      154|      219|jackfruit|\n",
      "|26-04-2018 02:58:37|   -33.88888|  151.27759|     0.0|     4111|     4550|    pizza|\n",
      "|28-04-2018 14:45:30| -37.8136276|144.9630576| -0.3078|       93|      477|    pizza|\n",
      "|29-04-2018 04:50:28| -33.8688197|151.2092955|  0.7543|      168|      206|    pizza|\n",
      "|29-04-2018 05:15:22|    -37.8823|      144.7|   0.296|      363|      472|    pizza|\n",
      "|29-04-2018 05:15:22|    -37.8823|      144.7|   0.296|      363|      472|  chicken|\n",
      "|28-04-2018 22:53:59| -37.4713077|144.7851531|  0.8674|      805|      608|    pizza|\n",
      "|28-04-2018 22:57:28|  -28.016667|      153.4|  0.1999|     1831|     2342|    pizza|\n",
      "|29-04-2018 07:49:26|  -27.999481|  153.42798|  0.6369|     2617|     1081|    pizza|\n",
      "|29-04-2018 03:17:22| -37.8136276|144.9630576|  0.8807|      804|      805|    pizza|\n",
      "|28-04-2018 10:31:07| -33.8688197|151.2092955|  0.4005|      154|      384|    pizza|\n",
      "+-------------------+------------+-----------+--------+---------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reform the dataframe to prepare for tranforming to json\n",
    "unioin_df = concat_df.union(df_with_food)\n",
    "unioin_df = unioin_df.drop('id')\n",
    "unioin_df = unioin_df.drop('timestamp')\n",
    "\n",
    "# get food type according to food class\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "generate_rev_dict()\n",
    "    \n",
    "def get_food_type(food_class):\n",
    "    the_class = str(food_class)\n",
    "    if the_class in rev_dict.keys():\n",
    "        return rev_dict[the_class]\n",
    "    return None\n",
    "\n",
    "get_food_type_udf = udf(get_food_type, StringType())\n",
    "unioin_df = unioin_df.withColumn('food', get_food_type_udf(unioin_df['food_class']))\n",
    "unioin_df = unioin_df.drop('food_class')\n",
    "\n",
    "print(unioin_df.count())\n",
    "unioin_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = unioin_df.toJSON()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"time\":\"29-04-2018 03:01:56\",\"lat\":-37.8136276,\"lng\":144.9630576,\"polarity\":0.0,\"followers\":3965,\"following\":4321,\"food\":\"pizza\"}'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert success\n"
     ]
    }
   ],
   "source": [
    "# insert data into couchdb\n",
    "my_db = Couch('test000')\n",
    "\n",
    "final_json = {}\n",
    "final_json[\"type\"] = \"FeatureCollection\"\n",
    "final_json[\"features\"] = []\n",
    "\n",
    "for row in json_data.collect():\n",
    "    \n",
    "    entry = {}\n",
    "    entry[\"type\"] = \"Feature\"\n",
    "    entry[\"properties\"] = {}\n",
    "    entry[\"geometry\"] = {}\n",
    "    entry[\"geometry\"][\"type\"] = \"Point\"\n",
    "    entry[\"geometry\"][\"coordinates\"] = []\n",
    "    \n",
    "    json_obj = json.loads(row)\n",
    "    entry[\"properties\"][\"time\"] = json_obj[\"time\"]\n",
    "    entry[\"properties\"][\"polarity\"] = json_obj[\"polarity\"]\n",
    "    entry[\"properties\"][\"followers\"] = json_obj[\"followers\"]\n",
    "    entry[\"properties\"][\"following\"] = json_obj[\"following\"]\n",
    "    entry[\"properties\"][\"food\"] = json_obj[\"food\"]\n",
    "    entry[\"geometry\"][\"coordinates\"].append(json_obj[\"lat\"])\n",
    "    entry[\"geometry\"][\"coordinates\"].append(json_obj[\"lng\"])\n",
    "    \n",
    "    final_json[\"features\"].append(entry)\n",
    "    \n",
    "my_db.insert(final_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under Precision/Recall (PR) curve: 5\n",
      "Area under Receiver Operating Characteristic (ROC) curve: 45.000\n",
      "Time to evaluate model: 0.185 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "metrics = BinaryClassificationMetrics(labels_and_predictions)\n",
    "print(\"Area under Precision/Recall (PR) curve: %.f\" % (metrics.areaUnderPR * 100))\n",
    "print(\"Area under Receiver Operating Characteristic (ROC) curve: %.3f\" % (metrics.areaUnderROC * 100))\n",
    "\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time to evaluate model: %.3f seconds\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
