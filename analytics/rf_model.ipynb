{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CCC Team 42, Melbourne\n",
    "\n",
    "Thuy Ngoc Ha    963370\n",
    "Lan Zhou        824371\n",
    "Zijian Wang     950618\n",
    "Ivan Chee       736901\n",
    "Duer Wang       824325\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "A ipythone notebook file used for demonstrating ml models\n",
    "\n",
    "Using random forest classification model for predicting food.\n",
    "Using random forest regression model for predicting homeless and homeless trend.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['SPARK_HOME'] = \"spark\"\n",
    "sys.path.append(\"spark/python\")\n",
    "sys.path.append(\"spark/python/lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coordinates of a given city\n",
    "import httplib2\n",
    "import json\n",
    "\n",
    "\n",
    "def cityPos(name):\n",
    "    url = \"https://maps.googleapis.com/maps/api/geocode/json?\" + \\\n",
    "          \"key=AIzaSyBsZErhxaT1oVgMrT-xGLcAN5nK3UHeGBU&address=\" + name\n",
    "    req = httplib2.Http(\".cache\")\n",
    "    resp, content = req.request(url, \"GET\")\n",
    "    res = json.loads(content)\n",
    "    return res[\"results\"][0][\"geometry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reform the data preparing for fitting the model\n",
    "import csv\n",
    "import json\n",
    "import codecs\n",
    "import time as t\n",
    "from couch import Couch\n",
    "\n",
    "COUCHDB_NAME = \"cl_richard\"\n",
    "REFORMED_FILE = \"data/output0.csv\"\n",
    "\n",
    "food_dict = {}\n",
    "rev_dict = {}\n",
    "\n",
    "def trans(path):\n",
    "    con = Couch(COUCHDB_NAME)\n",
    "    jsonData = con.query_all()\n",
    "\n",
    "    csvfile = open(REFORMED_FILE, 'w', newline='')\n",
    "    writer = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_ALL)\n",
    "    keys=['id', 'time', 'timestamp', 'lat', 'lng', 'polarity', 'followers', 'following', 'homeless', \\\n",
    "          'homeless_trend', 'food_class']\n",
    "    writer.writerow(keys)\n",
    "    \n",
    "    i = 0\n",
    "    for dic in jsonData:\n",
    "        # get coordinates\n",
    "        if dic['location']['coordinates'] is None:\n",
    "            city = dic['location']['place_name']\n",
    "            city = city.replace(\" \",\"%20\")\n",
    "            coor = cityPos(city)\n",
    "            lng = coor['location']['lng']\n",
    "            lat = coor['location']['lat']\n",
    "        else:\n",
    "            lng = dic['location']['coordinates'][0]\n",
    "            lat = dic['location']['coordinates'][1]\n",
    "            \n",
    "        # get time amd timesptamp\n",
    "        time = dic['created_at']['day']+'-'+trans_month(dic['created_at']['month'])+'-'+dic['created_at']['year']+\\\n",
    "                ' '+dic['created_at']['time']\n",
    "        timeArray = t.strptime(time, \"%d-%m-%Y %H:%M:%S\")\n",
    "        timestamp = t.mktime(timeArray)\n",
    "        \n",
    "        # to ensure at least one of homeless info and food info appears\n",
    "        home = dic['homeless']\n",
    "        foods = dic['food_list']\n",
    "        if (home is None) and (foods is None or len(foods) == 0):\n",
    "            continue\n",
    "        \n",
    "        # get homeless information\n",
    "        if home is None:\n",
    "            homeless = -1\n",
    "            homeless_trend = 0\n",
    "        else:\n",
    "            try:\n",
    "                homeless = dic['homeless']['cnt16']\n",
    "                homeless_trend = dic['homeless']['incre/decre']\n",
    "            except:\n",
    "                continue\n",
    "        # get food\n",
    "        if foods is None or len(foods) == 0:\n",
    "            writer.writerow([i, time, timestamp, lat, lng, dic['polarity'], dic['user']['followers'], \\\n",
    "                             dic['user']['following'], homeless, homeless_trend, \"-1\"])\n",
    "            i += 1\n",
    "        else:\n",
    "            for food in foods:\n",
    "                food_class = get_food_class(food)\n",
    "                writer.writerow([i, time, timestamp, lat, lng, dic['polarity'], dic['user']['followers'], \\\n",
    "                             dic['user']['following'], homeless, homeless_trend, food_class])\n",
    "                i += 1\n",
    "    csvfile.close()\n",
    "    \n",
    "def trans_month(month):\n",
    "    month_dic = {'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06', \\\n",
    "                 'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'}\n",
    "    return month_dic[month]\n",
    "\n",
    "def get_food_class(food):\n",
    "    if not food in food_dict.keys():\n",
    "        food_dict[food] = str(len(food_dict))\n",
    "    return food_dict[food]\n",
    "\n",
    "def generate_rev_dict():\n",
    "    for key,value in food_dict.items():\n",
    "        rev_dict[value] = key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"random forest model\"\n",
    "SPARK_URL = \"local[*]\"\n",
    "RANDOM_SEED = 12345\n",
    "TRAINING_DATA_RATIO = 0.7\n",
    "RF_NUM_TREES = 10\n",
    "RF_MAX_DEPTH = 5\n",
    "RF_NUM_BINS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(APP_NAME) \\\n",
    "    .master(SPARK_URL) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/duer/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: DeprecationWarning: generator 'CouchDatabase.__iter__' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 11885\n"
     ]
    }
   ],
   "source": [
    "# read data from couchdb and reform them into a dataframe\n",
    "trans(REFORMED_FILE)\n",
    "\n",
    "df = spark.read \\\n",
    "    .options(header = \"true\", inferschema = \"true\") \\\n",
    "    .csv(REFORMED_FILE)\n",
    "\n",
    "print(\"Total number of rows: %d\" % df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows without duplicates: 11885\n",
      "+----+-------------------+-------------+------------+------------+--------+---------+---------+--------+--------------+----------+\n",
      "|  id|               time|    timestamp|         lat|         lng|polarity|followers|following|homeless|homeless_trend|food_class|\n",
      "+----+-------------------+-------------+------------+------------+--------+---------+---------+--------+--------------+----------+\n",
      "|  15|22-12-2015 04:10:46|1.450717846E9| -37.8026085| 144.9475098|  0.6369|      629|      182|       4|            -9|        13|\n",
      "| 459|25-02-2016 07:38:13|1.456346293E9|-37.88408986| 144.9990552|  0.4019|       28|       64|     497|           115|        21|\n",
      "| 595|19-06-2017 22:08:47|1.497874127E9|-37.66839777|144.85187548|  0.3899|        1|        7|       4|            -9|         6|\n",
      "| 714|24-06-2017 08:01:46|1.498255306E9|   -37.84178|   144.93842|     0.0|     2220|     1175|       4|            -9|         0|\n",
      "|1319|18-07-2017 11:07:00| 1.50034002E9|    -37.7667|     144.967|  0.8126|      124|      331|       4|            -9|        28|\n",
      "|1412|22-07-2017 15:23:22|1.500701002E9|   -37.81623|   144.96829|     0.0|      225|      214|       4|            -9|         1|\n",
      "|1910|18-08-2017 10:49:26|1.503017366E9|-37.80930502|144.96896274|  0.6369|      617|      418|       4|            -9|         5|\n",
      "|2010|22-08-2017 11:17:51|1.503364671E9|   -37.81644|   144.98755|  0.5719|     1100|      812|       4|            -9|        60|\n",
      "|2178|29-08-2017 23:05:05|1.504011905E9| -37.8358572| 145.0931476|     0.0|    10111|     7303|       4|            -9|        71|\n",
      "|2254|01-09-2017 16:01:10| 1.50424567E9|   -37.81135|   144.96622|     0.0|      861|      324|       4|            -9|        34|\n",
      "|2434|11-09-2017 07:30:39|1.505079039E9|    -37.8167|     144.967|     0.0|     2288|     2424|       4|            -9|         7|\n",
      "|2547|16-09-2017 02:55:04|1.505494504E9|   -37.78446|    144.9695|  0.6908|      138|      205|       4|            -9|        19|\n",
      "|2831|30-09-2017 18:58:23|1.506761903E9|      -37.87|       145.1| -0.6486|      601|        1|       4|            -9|         6|\n",
      "|3688|30-12-2017 09:40:39|1.514587239E9|    -37.8167|     144.967|     0.0|     2283|     2440|       4|            -9|        23|\n",
      "|3831|11-01-2018 01:36:34|1.515594994E9|   -37.83795|   144.99571|     0.0|       22|      121|       4|            -9|        25|\n",
      "|3853|12-01-2018 19:15:43|1.515744943E9|      -37.87|       145.1| -0.6486|      670|        1|       4|            -9|         6|\n",
      "|3919|14-12-2014 13:35:57|1.418524557E9|-37.81648708|144.96038786|     0.0|      971|     1654|       4|            -9|        71|\n",
      "|4261|30-12-2014 04:56:38|1.419875798E9|-37.79398607|144.87369788|  0.4019|      110|      525|       4|            -9|       131|\n",
      "|4341|02-01-2015 13:50:28|1.420167028E9|-35.45011325|149.09339703| -0.3818|      461|       70|    1599|          -140|        28|\n",
      "|5174|09-02-2015 19:48:03|1.423471683E9|-37.86412094|144.98291549|     0.0|      308|      455|       4|            -9|        13|\n",
      "+----+-------------------+-------------+------------+------------+--------+---------+---------+--------+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "print(\"Total number of rows without duplicates: %d\" % df.count())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows having all information: 11875\n",
      "number of rows without food information: 10\n",
      "number of rows without homeless information: 0\n"
     ]
    }
   ],
   "source": [
    "# filter dataframe\n",
    "df_no_food = df.filter(df['food_class'] == -1)\n",
    "df_no_homeless = df.filter(df['homeless'] == -1)\n",
    "df_all_info = df.filter(df['food_class'] >= 0).filter(df['homeless'] >= 0)\n",
    "\n",
    "print(\"Number of rows having all information: %d\" % df_all_info.count())\n",
    "print(\"number of rows without food information: %d\" % df_no_food.count())\n",
    "print(\"number of rows without homeless information: %d\" % df_no_homeless.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training set rows: 8275\n",
      "Number of test set rows: 3600\n"
     ]
    }
   ],
   "source": [
    "# transform dataframe into RDD and split reformed data into tranning data and test data\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "transformed_df_food = df_all_info.rdd.map(lambda row: LabeledPoint(row[-1], Vectors.dense(row[2:-1])))\n",
    "transformed_df_homeless = df_all_info.rdd.map(lambda row: LabeledPoint(row[-3], Vectors.dense(row[2],row[3],row[4],row[5],row[6],row[7],row[10])))\n",
    "transformed_df_homeless_trend = df_all_info.rdd.map(lambda row: LabeledPoint(row[-2], Vectors.dense(row[2],row[3],row[4],row[5],row[6],row[7],row[10])))\n",
    "\n",
    "splits = [TRAINING_DATA_RATIO, 1.0 - TRAINING_DATA_RATIO]\n",
    "training_data_food, test_data_food = transformed_df_food.randomSplit(splits, RANDOM_SEED)\n",
    "training_data_homeless, test_data_homeless = transformed_df_homeless.randomSplit(splits, RANDOM_SEED)\n",
    "training_data_homeless_trend, test_data_homeless_trend = transformed_df_homeless_trend.randomSplit(splits, RANDOM_SEED)\n",
    "\n",
    "print(\"Number of training set rows: %d\" % training_data_food.count())\n",
    "print(\"Number of test set rows: %d\" % test_data_food.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train food classifier: 12.903 seconds\n"
     ]
    }
   ],
   "source": [
    "# train the classification model using training data\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from time import *\n",
    "\n",
    "start_time = time()\n",
    "num_classes = len(food_dict)\n",
    "\n",
    "model_food_classifier = RandomForest.trainClassifier(training_data_food, numClasses=num_classes, categoricalFeaturesInfo={}, \\\n",
    "    numTrees=RF_NUM_TREES, featureSubsetStrategy=\"auto\", impurity=\"gini\", \\\n",
    "    maxDepth=RF_MAX_DEPTH, maxBins=32, seed=RANDOM_SEED)\n",
    "\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time to train food classifier: %.3f seconds\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train homeless regressor: 10.204 seconds\n"
     ]
    }
   ],
   "source": [
    "# train the regression model using training data\n",
    "start_time = time()\n",
    "\n",
    "model_homeless_regressor = RandomForest.trainRegressor(training_data_homeless, categoricalFeaturesInfo={}, \\\n",
    "    numTrees=RF_NUM_TREES, featureSubsetStrategy=\"auto\", impurity=\"variance\", \\\n",
    "    maxDepth=RF_MAX_DEPTH, maxBins=32, seed=RANDOM_SEED)\n",
    "\n",
    "model_homeless_trend_regressor = RandomForest.trainRegressor(training_data_homeless_trend, categoricalFeaturesInfo={}, \\\n",
    "    numTrees=RF_NUM_TREES, featureSubsetStrategy=\"auto\", impurity=\"variance\", \\\n",
    "    maxDepth=RF_MAX_DEPTH, maxBins=32, seed=RANDOM_SEED)\n",
    "\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time to train homeless regressor: %.3f seconds\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Food classifier accuracy: 11.444%\n",
      "Homeless regressor accuracy: 53.833%\n",
      "Homeless trend regressor accuracy: 78.000%\n"
     ]
    }
   ],
   "source": [
    "# make predictions using test data and calculate the accuracy\n",
    "food_predictions = model_food_classifier.predict(test_data_food.map(lambda x: x.features))\n",
    "homeless_predictions = model_homeless_regressor.predict(test_data_homeless.map(lambda x: x.features))\n",
    "homeless_trend_predictions = model_homeless_trend_regressor.predict(test_data_homeless_trend.map(lambda x: x.features))\n",
    "\n",
    "\n",
    "labels_and_predictions_food = test_data_food.map(lambda x: x.label).zip(food_predictions)\n",
    "labels_and_predictions_homeless = test_data_homeless.map(lambda x: x.label).zip(homeless_predictions)\n",
    "labels_and_predictions_homeless_trend = test_data_homeless_trend.map(lambda x: x.label).zip(homeless_trend_predictions)\n",
    "\n",
    "food_acc = labels_and_predictions_food.filter(lambda x: x[0] == x[1]).count() / float(test_data_food.count())\n",
    "homeless_acc = labels_and_predictions_homeless.filter(lambda x: abs(x[0]-x[1]) < 10).count() / float(test_data_homeless.count())\n",
    "homeless_trend_acc = labels_and_predictions_homeless_trend.filter(lambda x: abs(x[0]-x[1]) < 10).count() / float(test_data_homeless_trend.count())\n",
    "\n",
    "print(\"Food classifier accuracy: %.3f%%\" % (food_acc * 100))\n",
    "print(\"Homeless regressor accuracy: %.3f%%\" % (homeless_acc * 100))\n",
    "print(\"Homeless trend regressor accuracy: %.3f%%\" % (homeless_trend_acc * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under Precision/Recall (PR) curve: 77\n",
      "Time to evaluate model: 2.901 seconds\n"
     ]
    }
   ],
   "source": [
    "# evaluation that I have no idea what it is\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "metrics = BinaryClassificationMetrics(labels_and_predictions_homeless_trend)\n",
    "print(\"Area under Precision/Recall (PR) curve: %.f\" % (metrics.areaUnderPR * 100))\n",
    "\n",
    "\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time to evaluate model: %.3f seconds\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_pre = df_no_food.count() > 0\n",
    "homeless_pre = df_no_homeless.count() > 0\n",
    "\n",
    "# make food predictions\n",
    "if food_pre:\n",
    "    transformed_df_no_food = df_no_food.rdd.map(lambda row: LabeledPoint(row[-1], Vectors.dense(row[2:-1])))\n",
    "    predict_foods = model_food_classifier.predict(transformed_df_no_food.map(lambda x: x.features))\n",
    "\n",
    "# make homeless predictions\n",
    "if homeless_pre:\n",
    "    transformed_df_no_homeless = df_no_homeless.rdd.map(lambda row: LabeledPoint(row[8], Vectors.dense(row[2],row[3],row[4],row[5],row[6],row[7],row[10])))\n",
    "    transformed_df_no_homeless_trend = df_no_homeless.rdd.map(lambda row: LabeledPoint(row[9], Vectors.dense(row[2],row[3],row[4],row[5],row[6],row[7],row[10])))\n",
    "    predict_homeless = model_homeless_regressor.predict(transformed_df_no_homeless.map(lambda x: x.features))\n",
    "    predict_homeless_trend = model_homeless_trend_regressor.predict(transformed_df_no_homeless_trend.map(lambda x: x.features))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip id with predictions\n",
    "if food_pre:\n",
    "    rdd_predict_foods = df_no_food.rdd.map(lambda row: row[0]).zip(predict_foods.map(int))\n",
    "    list_predict_foods = rdd_predict_foods.collect()\n",
    "if homeless_pre:\n",
    "    rdd_predict_homeless = df_no_homeless.rdd.map(lambda row: row[0]).zip(predict_homeless.map(int))\n",
    "    rdd_predict_homeless_trend = df_no_homeless.rdd.map(lambda row: row[0]).zip(predict_homeless_trend.map(int))\n",
    "    list_predict_homeless = rdd_predict_homeless.collect()\n",
    "    list_predict_homeless_trend = rdd_predict_homeless_trend.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform predicted rdd to dataframe and join it to original data that without food\n",
    "if food_pre:\n",
    "    df_predict_foods = spark.createDataFrame(list_predict_foods, schema=[\"id\",\"food_class\"])\n",
    "    df_no_food = df_no_food.drop('food_class')\n",
    "    concat_df_food = df_no_food.join(df_predict_foods, on='id')\n",
    "    \n",
    "if homeless_pre:\n",
    "    df_predict_homeless = spark.createDataFrame(list_predict_homeless, schema=[\"id\",\"homeless\"])\n",
    "    df_predict_homeless_trend = spark.createDataFrame(list_predict_homeless_trend, schema=[\"id\",\"homeless_trend\"])\n",
    "    \n",
    "    df_no_homeless = df_no_homeless.drop('homeless').drop('homeless_trend')\n",
    "    concat_df_homeless = df_no_homeless.join(df_predict_homeless, on='id').join(df_predict_homeless_trend, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11885\n",
      "+-------------------+------------+------------+--------+---------+---------+--------+--------------+---------+----------+\n",
      "|               time|         lat|         lng|polarity|followers|following|homeless|homeless_trend|     food|food_group|\n",
      "+-------------------+------------+------------+--------+---------+---------+--------+--------------+---------+----------+\n",
      "|22-12-2015 04:10:46| -37.8026085| 144.9475098|  0.6369|      629|      182|       4|            -9|    apple|    fruits|\n",
      "|25-02-2016 07:38:13|-37.88408986| 144.9990552|  0.4019|       28|       64|     497|           115|    beans|vegetables|\n",
      "|19-06-2017 22:08:47|-37.66839777|144.85187548|  0.3899|        1|        7|       4|            -9|  current|    fruits|\n",
      "|24-06-2017 08:01:46|   -37.84178|   144.93842|     0.0|     2220|     1175|       4|            -9|     pork|      meat|\n",
      "|18-07-2017 11:07:00|    -37.7667|     144.967|  0.8126|      124|      331|       4|            -9|   orange|    fruits|\n",
      "|22-07-2017 15:23:22|   -37.81623|   144.96829|     0.0|      225|      214|       4|            -9|    lemon|    fruits|\n",
      "|18-08-2017 10:49:26|-37.80930502|144.96896274|  0.6369|      617|      418|       4|            -9|  chicken|      meat|\n",
      "|22-08-2017 11:17:51|   -37.81644|   144.98755|  0.5719|     1100|      812|       4|            -9|      pea|vegetables|\n",
      "|29-08-2017 23:05:05| -37.8358572| 145.0931476|     0.0|    10111|     7303|       4|            -9|    mango|    fruits|\n",
      "|01-09-2017 16:01:10|   -37.81135|   144.96622|     0.0|      861|      324|       4|            -9|raspberry|    fruits|\n",
      "|11-09-2017 07:30:39|    -37.8167|     144.967|     0.0|     2288|     2424|       4|            -9| mushroom|vegetables|\n",
      "|16-09-2017 02:55:04|   -37.78446|    144.9695|  0.6908|      138|      205|       4|            -9|    salad|vegetables|\n",
      "|30-09-2017 18:58:23|      -37.87|       145.1| -0.6486|      601|        1|       4|            -9|  current|    fruits|\n",
      "|30-12-2017 09:40:39|    -37.8167|     144.967|     0.0|     2283|     2440|       4|            -9|   potato|vegetables|\n",
      "|11-01-2018 01:36:34|   -37.83795|   144.99571|     0.0|       22|      121|       4|            -9|     date|    fruits|\n",
      "|12-01-2018 19:15:43|      -37.87|       145.1| -0.6486|      670|        1|       4|            -9|  current|    fruits|\n",
      "|14-12-2014 13:35:57|-37.81648708|144.96038786|     0.0|      971|     1654|       4|            -9|    mango|    fruits|\n",
      "|30-12-2014 04:56:38|-37.79398607|144.87369788|  0.4019|      110|      525|       4|            -9|   caviar|   seafood|\n",
      "|02-01-2015 13:50:28|-35.45011325|149.09339703| -0.3818|      461|       70|    1599|          -140|   orange|    fruits|\n",
      "|09-02-2015 19:48:03|-37.86412094|144.98291549|     0.0|      308|      455|       4|            -9|    apple|    fruits|\n",
      "+-------------------+------------+------------+--------+---------+---------+--------+--------------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get food type according to food class\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from keywords import Keywords\n",
    "\n",
    "generate_rev_dict()\n",
    "    \n",
    "# get food name by food class\n",
    "def get_food_type(food_class):\n",
    "    the_class = str(food_class)\n",
    "    if the_class in rev_dict.keys():\n",
    "        return rev_dict[the_class]\n",
    "    return None\n",
    "get_food_type_udf = udf(get_food_type, StringType())\n",
    "\n",
    "# get food group by food name\n",
    "def get_food_group(food):\n",
    "    if food in Keywords.fastfood:\n",
    "        return \"fastfood\"\n",
    "    if food in Keywords.fruits:\n",
    "        return \"fruits\"\n",
    "    if food in Keywords.grains:\n",
    "        return \"grains\"\n",
    "    if food in Keywords.meat:\n",
    "        return \"meat\"\n",
    "    if food in Keywords.seafood:\n",
    "        return \"seafood\"\n",
    "    if food in Keywords.vegetables:\n",
    "        return \"vegetables\"\n",
    "    return None\n",
    "get_food_group_udf = udf(get_food_group, StringType())\n",
    "\n",
    "df_all_info = df_all_info.withColumn('food', get_food_type_udf(df_all_info['food_class']))\n",
    "df_all_info = df_all_info.drop('food_class')\n",
    "\n",
    "# reform the dataframe to prepare for tranforming to json\n",
    "if food_pre:\n",
    "    concat_df_food = concat_df_food.withColumn('food', get_food_type_udf(concat_df_food['food_class']))\n",
    "    concat_df_food = concat_df_food.drop('food_class')\n",
    "\n",
    "    union_df = df_all_info.union(concat_df_food)\n",
    "else:\n",
    "    union_df = df_all_info\n",
    "    \n",
    "    \n",
    "if homeless_pre:\n",
    "    concat_df_homeless = concat_df_homeless.withColumn('food', get_food_type_udf(concat_df_homeless['food_class']))\n",
    "    concat_df_homeless = concat_df_homeless.drop('food_class')\n",
    "    \n",
    "    union_df = union_df.union(concat_df_homeless)\n",
    "\n",
    "    \n",
    "union_df = union_df.drop('id')\n",
    "union_df = union_df.drop('timestamp')\n",
    "\n",
    "union_df = union_df.withColumn('food_group', get_food_group_udf(union_df['food']))\n",
    "\n",
    "print(union_df.count())\n",
    "union_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = union_df.toJSON()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"time\":\"22-12-2015 04:10:46\",\"lat\":-37.8026085,\"lng\":144.9475098,\"polarity\":0.6369,\"followers\":629,\"following\":182,\"homeless\":4,\"homeless_trend\":-9,\"food\":\"apple\",\"food_group\":\"fruits\"}'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert success\n",
      "11885\n"
     ]
    }
   ],
   "source": [
    "# insert data into couchdb\n",
    "my_db = Couch('prediction11885')\n",
    "\n",
    "final_json = {}\n",
    "final_json[\"type\"] = \"FeatureCollection\"\n",
    "final_json[\"features\"] = []\n",
    "\n",
    "i = 0\n",
    "for row in json_data.collect():\n",
    "    #print(i)\n",
    "    entry = {}\n",
    "    entry[\"type\"] = \"Feature\"\n",
    "    entry[\"properties\"] = {}\n",
    "    entry[\"geometry\"] = {}\n",
    "    entry[\"geometry\"][\"type\"] = \"Point\"\n",
    "    entry[\"geometry\"][\"coordinates\"] = []\n",
    "    \n",
    "    json_obj = json.loads(row)\n",
    "    entry[\"properties\"][\"time\"] = json_obj[\"time\"]\n",
    "    entry[\"properties\"][\"polarity\"] = json_obj[\"polarity\"]\n",
    "    entry[\"properties\"][\"followers\"] = json_obj[\"followers\"]\n",
    "    entry[\"properties\"][\"following\"] = json_obj[\"following\"]\n",
    "    entry[\"properties\"][\"food\"] = json_obj[\"food\"]\n",
    "    entry[\"properties\"][\"food_group\"] = json_obj[\"food_group\"]\n",
    "    entry[\"properties\"][\"homeless\"] = json_obj[\"homeless\"]\n",
    "    entry[\"properties\"][\"homeless_trend\"] = json_obj[\"homeless_trend\"]\n",
    "    entry[\"geometry\"][\"coordinates\"].append(json_obj[\"lat\"])\n",
    "    entry[\"geometry\"][\"coordinates\"].append(json_obj[\"lng\"])\n",
    "    \n",
    "    final_json[\"features\"].append(entry)\n",
    "    i += 1\n",
    "my_db.insert(final_json)\n",
    "print(i)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
